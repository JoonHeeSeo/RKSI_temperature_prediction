import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error

# ================================
# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° ë¶„ë¦¬
# ================================
# í•™ìŠµìš©: 2023ë…„
df_train = pd.read_csv("data/rksi_weather_2023.csv", parse_dates=["time"])
# í‰ê°€ìš©: 2024ë…„
df_test  = pd.read_csv("data/rksi_weather_2024.csv", parse_dates=["time"])

features = ["tmin", "tmax", "prcp", "wspd", "pres"]
target   = "tavg"

# ê²°ì¸¡ì¹˜ ì œê±°
df_train = df_train[features + [target]].dropna()
df_test  = df_test[features + [target]].dropna()

# ë…ë¦½ë³€ìˆ˜ì™€ ì¢…ì†ë³€ìˆ˜ ë¶„ë¦¬
X_train = df_train[features].values
y_train = df_train[[target]].values  # 2D array

X_test  = df_test[features].values
y_test  = df_test[[target]].values

# ================================
# 2. ì •ê·œí™” (Train ë°ì´í„° ê¸°ì¤€)
# ================================
scaler_X = StandardScaler().fit(X_train)
scaler_y = StandardScaler().fit(y_train)

X_train_scaled = scaler_X.transform(X_train)
y_train_scaled = scaler_y.transform(y_train)

X_test_scaled  = scaler_X.transform(X_test)
# y_testëŠ” í‰ê°€ ì‹œ ì—­ë³€í™˜ ë¹„êµë¥¼ ìœ„í•´ ìŠ¤ì¼€ì¼í•˜ì§€ ì•ŠìŒ

# Torch í…ì„œë¡œ ë³€í™˜
X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32)
y_train_t = torch.tensor(y_train_scaled, dtype=torch.float32)

# ================================
# 3. MLP ëª¨ë¸ ì •ì˜
# ================================
model = nn.Sequential(
    nn.Linear(len(features), 32),
    nn.ReLU(),
    nn.Linear(32, 16),
    nn.ReLU(),
    nn.Linear(16, 1)
)

criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# ================================
# 4. í•™ìŠµ ë£¨í”„
# ================================
epochs = 200
for epoch in range(1, epochs+1):
    y_pred_t = model(X_train_t)
    loss = criterion(y_pred_t, y_train_t)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if epoch % 20 == 0:
        print(f"Epoch {epoch}/{epochs}, Loss: {loss.item():.4f}")

# ================================
# 5. 2024 ë°ì´í„° ì˜ˆì¸¡ ë° ì—­ë³€í™˜
# ================================
model.eval()
with torch.no_grad():
    y_pred_test_scaled = model(torch.tensor(X_test_scaled, dtype=torch.float32)).numpy()

# ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë³µì›
y_pred_test = scaler_y.inverse_transform(y_pred_test_scaled)
y_true      = y_test  # already original scale

# ================================
# 6. í‰ê°€ ì§€í‘œ ê³„ì‚°
# ================================
# MAPE: í‰ê· ì ˆëŒ€ë°±ë¶„ìœ¨ì˜¤ì°¨
mape = mean_absolute_percentage_error(y_true, y_pred_test) * 100
# RMSE: í‰ê· ì œê³±ê·¼ì˜¤ì°¨
rmse = np.sqrt(mean_squared_error(y_true, y_pred_test))

print("\nğŸ“Š í‰ê°€ ê²°ê³¼:")
print(f"Mean Absolute Percentage Error (MAPE): {mape:.2f}%")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f} Â°C")

# ================================
# 7. ê²°ê³¼ ì˜ˆì‹œ ì¶œë ¥ (ì›í•˜ëŠ” ë§Œí¼ ì¶œë ¥)
# ================================
# ì˜ˆ: ì²˜ìŒ 5ê°œ ìƒ˜í”Œ ë¹„êµ
comparison = pd.DataFrame({
    "tavg_true": y_true.flatten(),
    "tavg_pred": y_pred_test.flatten(),
    "abs_err":    np.abs(y_true.flatten() - y_pred_test.flatten()),
    "pct_err":    np.abs((y_true.flatten() - y_pred_test.flatten()) / y_true.flatten()) * 100
})
print("\nì²« 5ê°œ ìƒ˜í”Œ ê²°ê³¼ ë¹„êµ:")
print(comparison.head())
